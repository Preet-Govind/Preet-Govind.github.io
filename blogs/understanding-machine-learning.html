<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-ML notes</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
	    <header>
        <h2>Understanding Machine Learning</h2>
    </header>
    <main>
        <!--<article>
            <h2>Introduction</h2>
            <p>
            </p>
             More content 
        </article>-->
        
         <section class="section">
        <h2>Regression</h2>
        <p>In regression tasks, the goal is to predict a continuous value. The error is based on the distance between predicted and actual values. Here are three common loss functions:</p>

        <h3>Mean Error (ME)</h3>
        <p>Formula: <code>ME = (1/n) ∑ (y_i - ŷ_i)</code></p>
        <p>Description: Averages errors which can cancel each other out. Not commonly used due to this cancellation effect.</p>

        <h3>Mean Squared Error (MSE)</h3>
        <p>Formula: <code>MSE = (1/n) ∑ (y_i - ŷ_i)²</code></p>
        <p>Description: Squares errors, penalizing larger deviations more. Sensitive to outliers.</p>
        <pre><code>def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)</code></pre>

        <h3>Mean Absolute Error (MAE)</h3>
        <p>Formula: <code>MAE = (1/n) ∑ |y_i - ŷ_i|</code></p>
        <p>Description: Computes absolute differences, robust to outliers.</p>
        <pre><code>def mean_absolute_error(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))</code></pre>
    </section>

    <section class="section">
        <h2>Classification</h2>
        <p>In classification tasks, the goal is to predict discrete categories. Common loss functions include:</p>

        <h3>Binary Classification</h3>
        <p>Used for two classes (e.g., yes/no). Common loss function is Binary Cross-Entropy Loss.</p>
        <pre><code>from sklearn.metrics import log_loss
# Example usage
loss = log_loss(y_true, y_pred)</code></pre>

        <h3>Multi-class Classification</h3>
        <p>Involves multiple classes (e.g., cat, dog, bird). Common loss function is Categorical Cross-Entropy Loss.</p>
        <pre><code>from sklearn.metrics import log_loss
# Example usage
loss = log_loss(y_true, y_pred, labels=[0, 1, 2])</code></pre>
    </section>

    <section class="section">
        <h2>Cost Function</h2>
        <p>The cost function averages the error across all data points. It helps optimize model parameters. The form of the cost function can vary, but the focus is on minimizing the error.</p>
    </section>

    <section class="section">
        <h2>Types of Machine Learning Algorithms</h2>
        <h3>Supervised Learning</h3>
        <p>Data includes input <code>x</code> and output label <code>y</code>. Includes regression and classification tasks.</p>

        <h3>Unsupervised Learning</h3>
        <p>Data includes only input <code>x</code>. Includes clustering, anomaly detection, and dimensionality reduction.</p>
    </section>

    <section class="section">
        <h2>Hypothesis in Machine Learning</h2>
        <h3>Hypothesis</h3>
        <p>A machine learning hypothesis is a candidate model that approximates a target function for mapping inputs to outputs.</p>

        <h3>Hypothesis Set</h3>
        <p>This is the space of possible hypotheses for mapping inputs to outputs, influenced by problem framing, model choice, and configuration.</p>
    </section>
    </main>
    <footer>
        <p><a href="../index.html">Back to Portfolio</a></p>
    </footer>
</body>
</html>
